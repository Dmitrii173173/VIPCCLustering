{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# # Определение числа ближайших соседей\n",
    "# n_neighbors = 1\n",
    "\n",
    "# # Поиск ближайших соседей для каждой точки\n",
    "# nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto').fit(cut_scaled_points_np)\n",
    "# distances, indices = nbrs.kneighbors(cut_scaled_points_np)\n",
    "\n",
    "# # Вычисление среднего расстояния до n_neighbors точек для каждой точки\n",
    "# avg_distances = np.mean(distances, axis=1)\n",
    "\n",
    "# # Определение порогового значения для удаления редких точек (можно настроить)\n",
    "# threshold_low = np.percentile(avg_distances, 2)\n",
    "\n",
    "# # Отфильтровать точки по пороговому значению\n",
    "# filtered_indices = np.where(avg_distances <= threshold_low)[0]\n",
    "# filtered_cut_points_np = cut_points_np[filtered_indices]\n",
    "# filtered_cut_scaled_points_np = cut_scaled_points_np[filtered_indices]\n",
    "\n",
    "# # Произвести повторное масштабирование, если это необходимо\n",
    "# # filtered_scaled_points_np = scaler.fit_transform(filtered_cut_points_np)\n",
    "\n",
    "# # Применение DBSCAN на отфильтрованных точках\n",
    "# eps = 0.2  # Пример значения для параметра epsilon\n",
    "# min_samples = 20  # Пример значения для параметра min_samples\n",
    "# dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "# cut_labels = dbscan.fit_predict(filtered_cut_scaled_points_np)\n",
    "\n",
    "# # Создание вектора цветов для обрезанных точек\n",
    "# cut_num_points = len(filtered_cut_points_np)\n",
    "# cut_colors = np.zeros((cut_num_points, 3))\n",
    "# for label in np.unique(cut_labels):\n",
    "#     if label == -1:\n",
    "#         continue\n",
    "#     mask = cut_labels == label\n",
    "#     cut_colors[mask] = np.random.rand(1, 3)\n",
    "\n",
    "# # Создание облака точек после обрезки\n",
    "# cut_pcd = o3d.geometry.PointCloud()\n",
    "# cut_pcd.points = o3d.utility.Vector3dVector(filtered_cut_points_np)\n",
    "# cut_pcd.colors = o3d.utility.Vector3dVector(cut_colors)\n",
    "\n",
    "# # Визуализация кластеров после обрезки\n",
    "# o3d.visualization.draw_geometries([cut_pcd])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutting_height = -1.2  # Значение для отсечения снизу\n",
    "\n",
    "# # Отсечение точек снизу по оси Z\n",
    "# cut_indices = np.where(points_np[:, 2] > cutting_height)[0]\n",
    "# cut_points_np = points_np[cut_indices]\n",
    "# cut_scaled_points_np = scaled_points_np[cut_indices]\n",
    "# cut_labels = labels[cut_indices]\n",
    "\n",
    "# # Создание вектора цветов для обрезанных точек\n",
    "# cut_num_points = len(cut_points_np)\n",
    "# cut_colors = np.zeros((cut_num_points, 3))\n",
    "# for label in np.unique(cut_labels):\n",
    "#     if label == -1:\n",
    "#         continue\n",
    "#     mask = cut_labels == label\n",
    "#     cut_colors[mask] = np.random.rand(1, 3)\n",
    "\n",
    "# # Создание облака точек после обрезки\n",
    "# cut_pcd = o3d.geometry.PointCloud()\n",
    "# cut_pcd.points = o3d.utility.Vector3dVector(cut_points_np)\n",
    "# cut_pcd.colors = o3d.utility.Vector3dVector(cut_colors)\n",
    "\n",
    "# # Визуализация кластеров после обрезки\n",
    "# o3d.visualization.draw_geometries([cut_pcd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "clustered_ply_file = 'D:/Cluster_proj/VIPCClustering/data/KITTI/velodyne/000008.bin'\n",
    "# Загрузка данных из файла\n",
    "with open(clustered_ply_file, 'rb') as f:\n",
    "    # Чтение данных из файла в массив numpy\n",
    "    velo_points = np.fromfile(f, dtype=np.float32).reshape(-1, 4)\n",
    "# Загрузка облака точек из файла .bin\n",
    "velo_points = np.fromfile(clustered_ply_file, dtype=np.float32).reshape(-1, 4)\n",
    "points_np = velo_points[:, :3]\n",
    "\n",
    "\n",
    "\n",
    "# Загрузка облака точек\n",
    "# pcd = o3d.io.read_point_cloud(clustered_ply_file)\n",
    "# points_np = np.asarray(pcd.points)\n",
    "\n",
    "# Масштабирование данных\n",
    "scaler = StandardScaler()\n",
    "scaled_points_np = scaler.fit_transform(points_np)\n",
    "\n",
    "# Определение высоты отсечения\n",
    "cutting_height = -1.2  # Значение для отсечения снизу\n",
    "\n",
    "# Отсечение точек снизу по оси Z\n",
    "cut_indices = np.where(points_np[:, 2] > cutting_height)[0]\n",
    "cut_points_np = points_np[cut_indices]\n",
    "cut_scaled_points_np = scaled_points_np[cut_indices]\n",
    "\n",
    "# Определение числа ближайших соседей\n",
    "n_neighbors = 1\n",
    "\n",
    "# Поиск ближайших соседей для каждой точки\n",
    "nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto').fit(cut_scaled_points_np)\n",
    "distances, indices = nbrs.kneighbors(cut_scaled_points_np)\n",
    "\n",
    "# Вычисление среднего расстояния до n_neighbors точек для каждой точки\n",
    "avg_distances = np.mean(distances, axis=1)\n",
    "\n",
    "# Определение порогового значения для удаления редких точек (можно настроить)\n",
    "threshold_low = np.percentile(avg_distances, 1)\n",
    "\n",
    "# Отфильтровать точки по пороговому значению\n",
    "filtered_indices = np.where(avg_distances <= threshold_low)[0]\n",
    "filtered_cut_points_np = cut_points_np[filtered_indices]\n",
    "filtered_cut_scaled_points_np = cut_scaled_points_np[filtered_indices]\n",
    "\n",
    "# Применение DBSCAN на отфильтрованных точках\n",
    "eps = 0.1  # Пример значения для параметра epsilon\n",
    "min_samples = 10  # Пример значения для параметра min_samples\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "cut_labels = dbscan.fit_predict(filtered_cut_scaled_points_np)\n",
    "\n",
    "# Создание вектора цветов для обрезанных точек\n",
    "cut_num_points = len(filtered_cut_points_np)\n",
    "cut_colors = np.zeros((cut_num_points, 3))\n",
    "for label in np.unique(cut_labels):\n",
    "    if label == -1:\n",
    "        continue\n",
    "    mask = cut_labels == label\n",
    "    cut_colors[mask] = np.random.rand(1, 3)\n",
    "\n",
    "# Создание облака точек после обрезки\n",
    "cut_pcd = o3d.geometry.PointCloud()\n",
    "cut_pcd.points = o3d.utility.Vector3dVector(filtered_cut_points_np)\n",
    "cut_pcd.colors = o3d.utility.Vector3dVector(cut_colors)\n",
    "\n",
    "# Визуализация кластеров после обрезки\n",
    "o3d.visualization.draw_geometries([cut_pcd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "\n",
    "clustered_ply_file = 'D:/Cluster_proj/VIPCClustering/data/KITTI/velodyne/000008.bin'\n",
    "# Загрузка данных из файла\n",
    "with open(clustered_ply_file, 'rb') as f:\n",
    "    # Чтение данных из файла в массив numpy\n",
    "    velo_points = np.fromfile(f, dtype=np.float32).reshape(-1, 4)\n",
    "# Загрузка облака точек из файла .bin\n",
    "velo_points = np.fromfile(clustered_ply_file, dtype=np.float32).reshape(-1, 4)\n",
    "points_np = velo_points[:, :3]\n",
    "\n",
    "\n",
    "\n",
    "# Загрузка облака точек\n",
    "# clustered_ply_file = 'clustered_point_cloud.ply'\n",
    "# pcd = o3d.io.read_point_cloud(clustered_ply_file)\n",
    "# points_np = np.asarray(pcd.points)\n",
    "\n",
    "# Масштабирование данных\n",
    "scaler = StandardScaler()\n",
    "scaled_points_np = scaler.fit_transform(points_np)\n",
    "\n",
    "# Определение высоты отсечения\n",
    "cutting_height = -1.2  # Значение для отсечения снизу\n",
    "\n",
    "# Отсечение точек снизу по оси Z\n",
    "cut_indices = np.where(points_np[:, 2] > cutting_height)[0]\n",
    "cut_points_np = points_np[cut_indices]\n",
    "cut_scaled_points_np = scaled_points_np[cut_indices]\n",
    "\n",
    "# Кластеризация DBSCAN для обрезанных точек\n",
    "eps = 0.2  # Пример значения для параметра epsilon\n",
    "min_samples = 5 # Пример значения для параметра min_samples\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "cut_labels = dbscan.fit_predict(cut_scaled_points_np)\n",
    "\n",
    "# Создание вектора цветов для обрезанных точек\n",
    "cut_num_points = len(cut_points_np)\n",
    "cut_colors = np.zeros((cut_num_points, 3))\n",
    "for label in np.unique(cut_labels):\n",
    "    if label == -1:\n",
    "        continue\n",
    "    mask = cut_labels == label\n",
    "    cut_colors[mask] = np.random.rand(1, 3)\n",
    "\n",
    "# Создание облака точек после обрезки\n",
    "cut_pcd = o3d.geometry.PointCloud()\n",
    "cut_pcd.points = o3d.utility.Vector3dVector(cut_points_np)\n",
    "cut_pcd.colors = o3d.utility.Vector3dVector(cut_colors)\n",
    "\n",
    "# Визуализация кластеров после обрезки\n",
    "o3d.visualization.draw_geometries([cut_pcd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: -0.09813004\n",
      "Dunn Index calculation requires the 'pyclustertend' package, please install it to calculate Dunn Index.\n",
      "Davies-Bouldin Index: 1.3492844040819187\n",
      "C-Index calculation requires the 'cluster-validation' package, please install it to calculate C-Index.\n",
      "Calinski–Harabasz Index: 968.9286820565736\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_avg = silhouette_score(filtered_cut_scaled_points_np, cut_labels)\n",
    "print(\"Silhouette Score:\", silhouette_avg)\n",
    "\n",
    "# Dunn Index (requires installation of `pyclustertend` package)\n",
    "try:\n",
    "    from pyclustertend import dunn\n",
    "    dunn_index = dunn(filtered_cut_scaled_points_np, cut_labels)\n",
    "    print(\"Dunn Index:\", dunn_index)\n",
    "except ImportError:\n",
    "    print(\"Dunn Index calculation requires the 'pyclustertend' package, please install it to calculate Dunn Index.\")\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "davies_bouldin_index = davies_bouldin_score(filtered_cut_scaled_points_np, cut_labels)\n",
    "print(\"Davies-Bouldin Index:\", davies_bouldin_index)\n",
    "\n",
    "# C-Index (requires installation of `cluster-validation` package)\n",
    "try:\n",
    "    from clusteval import c_index\n",
    "    c_index_value = c_index(filtered_cut_scaled_points_np, cut_labels)\n",
    "    print(\"C-Index:\", c_index_value)\n",
    "except ImportError:\n",
    "    print(\"C-Index calculation requires the 'cluster-validation' package, please install it to calculate C-Index.\")\n",
    "\n",
    "# Calinski–Harabasz Index\n",
    "calinski_harabasz_index = calinski_harabasz_score(filtered_cut_scaled_points_np, cut_labels)\n",
    "print(\"Calinski–Harabasz Index:\", calinski_harabasz_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: -0.09813004\n",
      "Dunn Index calculation requires the 'pyclustertend' package, please install it to calculate Dunn Index.\n",
      "Davies-Bouldin Index: 1.3492844040819187\n",
      "C-Index calculation requires the 'cluster-validation' package, please install it to calculate C-Index.\n",
      "Calinski–Harabasz Index: 968.9286820565736\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_avg = silhouette_score(filtered_cut_scaled_points_np, cut_labels)\n",
    "print(\"Silhouette Score:\", silhouette_avg)\n",
    "\n",
    "# Silhouette:\n",
    "# The silhouette score measures how similar an object is to its own cluster compared to other clusters. \n",
    "# It ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster \n",
    "# and poorly matched to neighboring clusters.\n",
    "\n",
    "# Dunn Index (requires installation of `pyclustertend` package)\n",
    "try:\n",
    "    from pyclustertend import dunn\n",
    "    dunn_index = dunn(filtered_cut_scaled_points_np, cut_labels)\n",
    "    print(\"Dunn Index:\", dunn_index)\n",
    "except ImportError:\n",
    "    print(\"Dunn Index calculation requires the 'pyclustertend' package, please install it to calculate Dunn Index.\")\n",
    "\n",
    "# Dunn Index:\n",
    "# The Dunn index measures the compactness and separation of clusters. \n",
    "# It is defined as the ratio of the smallest distance between points not in the same cluster to the \n",
    "# largest intra-cluster distance.\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "davies_bouldin_index = davies_bouldin_score(filtered_cut_scaled_points_np, cut_labels)\n",
    "print(\"Davies-Bouldin Index:\", davies_bouldin_index)\n",
    "\n",
    "# Davies-Bouldin Index:\n",
    "# The Davies-Bouldin index measures the average similarity between each cluster and its most similar cluster. \n",
    "# Lower values of the Davies-Bouldin index indicate better clustering.\n",
    "\n",
    "# C-Index (requires installation of `cluster-validation` package)\n",
    "try:\n",
    "    from clusteval import c_index\n",
    "    c_index_value = c_index(filtered_cut_scaled_points_np, cut_labels)\n",
    "    print(\"C-Index:\", c_index_value)\n",
    "except ImportError:\n",
    "    print(\"C-Index calculation requires the 'cluster-validation' package, please install it to calculate C-Index.\")\n",
    "\n",
    "# Calinski–Harabasz Index\n",
    "calinski_harabasz_index = calinski_harabasz_score(filtered_cut_scaled_points_np, cut_labels)\n",
    "print(\"Calinski–Harabasz Index:\", calinski_harabasz_index)\n",
    "\n",
    "# Calinski–Harabasz Index:\n",
    "# The Calinski–Harabasz index measures the ratio of between-cluster dispersion to within-cluster dispersion. \n",
    "# Higher values indicate better clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEVI Index: 14.145626\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cevi_index(X, labels):\n",
    "    clusters = np.unique(labels)\n",
    "    cevi_values = []\n",
    "    for cluster in clusters:\n",
    "        cluster_points = X[labels == cluster]\n",
    "        if len(cluster_points) <= 1:\n",
    "            cevi_values.append(0)  # Not enough data for diversity estimation\n",
    "        else:\n",
    "            cluster_variance = np.var(cluster_points, axis=0)\n",
    "            cevi_values.append(np.sum(cluster_variance))\n",
    "\n",
    "    return np.mean(cevi_values)\n",
    "\n",
    "# Compute CEVI Index\n",
    "\n",
    "cevi_value = cevi_index(filtered_cut_points_np, cut_labels)\n",
    "print(\"CEVI Index:\", cevi_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variety Index: 1.1655959\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def variety_index(X, labels):\n",
    "    clusters = np.unique(labels)\n",
    "    diversity_per_cluster = []\n",
    "    for cluster in clusters:\n",
    "        cluster_points = X[labels == cluster]\n",
    "        if len(cluster_points) <= 1:\n",
    "            diversity_per_cluster.append(0)  # Not enough data to assess diversity\n",
    "        else:\n",
    "            distances = np.linalg.norm(cluster_points[:, None] - cluster_points, axis=-1)\n",
    "            diversity_per_cluster.append(np.std(distances))\n",
    "\n",
    "    return np.mean(diversity_per_cluster)\n",
    "\n",
    "# Calculation of \"Variety Index\"\n",
    "variety_index_value = variety_index(filtered_cut_points_np, cut_labels)\n",
    "print(\"Variety Index:\", variety_index_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda117",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
